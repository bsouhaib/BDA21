{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Big Data Analytics 2020-2021 - Assignment II (Solution)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem 1.12"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(a)\n",
    "\n",
    "$\\frac{\\mathrm{d} E_{\\text{in}}(h)}{\\mathrm{d} h} = 0$ has solution $h_{\\text{mean}} = \\frac1N \\sum_{n = 1}^N y_n$, which is a minimum since $E_{\\text{in}}^{''}(h_{\\text{mean}}) > 0$.\n",
    "\n",
    "(b) \n",
    "\n",
    "Using the fact that $\\frac{\\mathrm{d} \\left | x \\right | }{\\mathrm{d} x} = \\operatorname{sign} \\left( x \\right) ~ (x \\neq 0)$, we obtain $\\frac{\\mathrm{d} E_{\\text{in}}(h)}{\\mathrm{d} h} = 0$ implies $\\sum_{n = 1}^{N} \\operatorname{sign} \\left( {y}_{n} - h \\right) = 0$. This equals to zero only when the number of positive items equals the number of negative, i.e. $h_{med} = \\operatorname{median} \\left\\{ {y}_{1}, {y}_{2}, \\cdots, {y}_{N} \\right\\}$.\n",
    "\n",
    "Note that a more rigorous proof can be obtained by considering various cases.\n",
    "\n",
    "(c)\n",
    "\n",
    " If $y_N$ is perturbed to $y_N + \\epsilon$, where $\\epsilon \\to \\infty$, we can see that $h_{mean}$ will increase a lot since $y_N$ contributes to its calculation, while $h_{med}$ is not affected by this outlier.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem 3.12\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "\n",
    "In the finite-dimensional case, a square matrix $P$ is called a projection matrix if it is equal to its square, i.e., if $P = P^2$.\n",
    "\n",
    "$H^2 = X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T = H$ so $H$ is a projection matrix. \n",
    "\n",
    "$\\hat{y}$ is the projection of $y$ onto the space spanned by $X$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Problem 3.17"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We consider the function $E(u,v) = e^{u} + e^{2v} + e^{uv} + u^2 - 3 uv + 4 v^2 - 3u -5v$ where $u,v \\in \\mathbb{R}$.\n",
    "\n",
    "(a)\n",
    "\n",
    "We have \n",
    "\n",
    "$$\n",
    "\\hat{E}_1(\\Delta u,\\Delta v) = E(0,0) + (\\nabla E(0,0))^T \\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where we used the first-order Taylor expansion of $E(\\Delta u,\\Delta v)$ around $(0,0)$, given by\n",
    "\n",
    "$$\n",
    "E(\\Delta u,\\Delta v) \\approx E(0,0) + (\\nabla E(0,0))^T \\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The gradient is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E(u,v) &= \\begin{pmatrix}\n",
    "\\underline{\\partial E(u,v)} \\\\\n",
    "\\partial u\\\\\n",
    "\\underline{\\partial E(u,v)} \\\\\n",
    "\\partial v\n",
    "\\end{pmatrix}  = \\begin{pmatrix}\n",
    "e^u + v e^{uv} + 2u -3v -3 \\\\\n",
    "2 e^{2v} + u e^{uv} - 3u + 8 v -5\n",
    "\\end{pmatrix} \n",
    "\\end{align*}\n",
    "and\n",
    "$$\n",
    "\\nabla E(0,0)= \\begin{pmatrix}\n",
    "-2 \\\\\n",
    "-3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Hence, we obtain\n",
    "$$\n",
    "\\hat{E}_1(\\Delta u,\\Delta v) = 3 - 2 \\Delta u -3 \\Delta v \n",
    "$$\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(b)\n",
    "\n",
    "The solution is given by the negative gradient, $-\\nabla E(u,v)$. By addition the constraint, we obtain $\\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix} = -0.5 \\frac{\\nabla E(u,v)}{\\|\\nabla E(u,v)\\|_2} $, which gives $\\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix}  = \\begin{pmatrix}\n",
    "\\frac{1}{\\sqrt{13} }\\\\\n",
    "\\frac{3}{2 \\sqrt{13} }\n",
    "\\end{pmatrix} $\n",
    "\n",
    "Hence, we have $E(\\Delta u,\\Delta v) \\approx 2.25 $.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(c)\n",
    "\n",
    "We need the Hessian, which is given by \n",
    "\n",
    "$$\n",
    "\\nabla^2 E(u,v) = \\begin{pmatrix}\n",
    "e^u + v^2 e^{uv} + 2 & e^{uv} + uv e^{uv} -3\\\\\n",
    "e^{uv} + uv e^{uv} -3 & 4 e^{2v} + u^2 e^{uv} + 8\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla^2 E(0,0) = \\begin{pmatrix}\n",
    "3 & -2\\\\\n",
    "-2 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We obtain\n",
    "$$\n",
    "\\hat{E}_2(\\Delta u,\\Delta v) = 3 - 2 \\Delta u -3 \\Delta v + \\frac{3}{2} (\\Delta u)^2 - 2 \\Delta u\\Delta v + 6 (\\Delta v)^2.\n",
    "$$\n",
    "and\n",
    "$b_{uu} = \\frac{3}{2}$, $b_{vv} = 6$, $b_{uv}=-2$, $b_u = -2$, $b_v = -3$, $b=3$.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(d)\n",
    "\n",
    "A stationary point is given by solving\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\hat{E_2}(\\Delta u, \\Delta v) = \\begin{pmatrix}\n",
    "0\\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\end{align*}\n",
    "or, equivalently,\n",
    "$$\n",
    " \\begin{cases}\n",
    "3 \\Delta u -2 \\Delta v = 2\\\\\n",
    "-2 \\Delta u + 12 \\Delta v = 3\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The solution is given by\n",
    "$\\begin{pmatrix}\n",
    "\\Delta u^* \\\\\n",
    "\\Delta v^* \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{15}{16}\\\\\n",
    "\\frac{13}{32} \n",
    "\\end{pmatrix}$\n",
    "\n",
    "The previous linear system can be rewritten as\n",
    "\\begin{pmatrix}\n",
    "3 & -2 \\\\\n",
    "-2 & 12 \n",
    "\\end{pmatrix}  \\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "which is equivalent to\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{pmatrix}\n",
    "\\Delta u^*\\\\\n",
    "\\Delta v^*\n",
    "\\end{pmatrix} = - (\\nabla^2 E(0,0))^{-1} \\nabla E(0,0).\n",
    "\\end{equation}\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "(e)\n",
    "\n",
    "1. For the Newton direction, we obtain\n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix} = (0.4588, 0.1988)^T\n",
    "$$\n",
    "\n",
    "with an error of $1.8905$.\n",
    "\n",
    "2. We obtain $\\theta = 1.0575$ and\n",
    "$$ \n",
    "\\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix} = (0.4356, 0.2455)^T\n",
    "$$\n",
    "\n",
    "with an error of $1.8684$.\n",
    "\n",
    "We previously obtained \n",
    "\n",
    "$$ \n",
    "\\begin{pmatrix}\n",
    "\\Delta u\\\\\n",
    "\\Delta v\n",
    "\\end{pmatrix} = (0.2773, 0.416)^T\n",
    "$$\n",
    "with an error of $2.2508$.\n",
    "\n",
    "We obtain better results with the Newton's method.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient descent (proof)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "See pdf file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}