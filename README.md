# Big Data Analytics at UMONS
This is a repository to organize the teaching material for [Big Data Analytics](http://applications.umons.ac.be/web/en/pde/2020-2021/aa/S-INFO-075.htm), to be taught by [Souhaib Ben Taieb](http://www.souhaib-bentaieb.com).

# Course links

- [Moodle (for discussion forum, assignments)](https://moodle.umons.ac.be/course/view.php?id=2786s)

# Project

- [Project 2020-2021](project/project.pdf)

# Content

<!--- Lectures: 16 - Labs: 17 (30, 30) --->

- Week 1 (Feb. 1-5). 
  - Lecture 1 (Feb. 1): [Introduction and the perceptron learning model](./slides/1-bda-perceptron.pdf)
  - Lecture 2 (Feb. 4): The learning problem

- Week 2 (Feb. 8-12). 
  - Lab 1 (Feb. 9): [The perceptron learning model](./labs/1-perceptron/perceptron.pdf)
  - Lab 2 (Feb. 10): The perceptron learning model (continued).
  
- Week 3 (Feb. 15-19).
  - No class.

- Week 4 (Feb. 22-26).
  - Lecture 3 (Feb. 23): [Learning theory (bin model, Hoeffding)](./slides/2-bda-learning-1.pdf)
  - Lab 3 (Feb. 24): [The learning problem](./labs/2-learning/learning.pdf)
  - Lab 4 (Feb. 25): The learning problem (continued).

- Week 4 (Mar. 01-07).
  - Lecture 4 (Mar. 3): [Learning theory (dichotomies, growth function)](./slides/2-bda-learning-2.pdf)
  - Lecture 5 (Mar. 4): [Learning theory (dichotomies, growth function)](./slides/2-bda-learning-3.pdf)
  - Lecture 6 (Mar. 5): [Learning theory (break point, VC dimension)](./slides/2-bda-learning-4.pdf)

- Week 5 (Mar. 08-14).
  - Lab 5 (Mar. 10): [Learning theory](./labs/2-learning/learning2.pdf)
  - Lecture 7 (Mar. 11): Learning theory (VC dimension of perceptrons, interpretation of VC dimension, etc)

- Week 6 (Mar. 15-21).
  - [Assignment I](./assignments/assignment1.pdf) 

  - Lab 6 (Mar. 15): [Learning theory (continued)](./labs/2-learning/learning2.pdf)
  
  - Lecture 8 (Mar. 19): [Bias and variance analysis](./slides/2-bda-learning-5.pdf)


- Week 7 (Mar. 22-28).

    <!--- FINISH BIAS AND VARIANCE + (learning 2) problem 2.3 --->
  - Lab 7 (Mar. 22): Learning theory (continued)

    <!--- LINEAR MODELS  --->
  - Lecture 9 (Mar. 25): [Linear models](./slides/3-linear-model.pdf)


    <!--- Gradient descent --->
  - Lecture 10 (Mar. 25): [Gradient descent](./slides/3-linear-model.pdf)



- Week 8 (Mar. 29 - Apr. 4).

  <!--- Linear regression - Ex 3.3, 3.4, 3.11, 3.14, 3.15--->
  - Lab 8 (March. 31): [Linear regression](./labs/3-linear-model/linear-regression.pdf)

  <!--- Linear classfiication - 3.7, 3.9, 3.10, 3.4  + Hessian of logistic regression (see notes) --->
  - Lab 9 (Apr. 1): Linear regression (continued)

  - Lab 10 (Apr. 2): [Linear classification](./labs/3-linear-model/linear-classification.pdf)

  <!--- Convergence proofs of gradient descent --->
  <!--- https://raghumeka.github.io/CS289ML/gdnotes.pdf --->
  <!---  https://www.stat.cmu.edu/~ryantibs/convexopt-F13/scribes/lec6.pdf --->
  <!---  APPLIED EXERCISES --->


  <!--- 3.5?--->
  - [Assignment II](./assignments/assignment2.pdf) 

<!---
  * Graph Neural Networks - https://www.cs.mcgill.ca/~wlh/grl_book/
  * Multitask Learning
  * Fairness in machine learning: https://fairmlclass.github.io/
  * https://fairmlbook.org/
  * Optimization in deep learning: https://www.deeplearningbook.org/
  * Reccurent Neural Networks
https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture05.pdf
  --->

**Spring Break**

<!--- Lecture 11 12 (Neural networks, KERAS) 13 14 15 (more on NN?, regularization, kernel learning/trick?, SVM?) --->
<!---  Lab 13, 14, 15 (Neural Networks) --->

- Week 9 (Apr. 19-25).
 - Lab 11 (Apr. 21): Linear classification (continued)
 - Lab 12 (Apr. 23): Linear classification (continued)


- Week 10 (Apr. 26 - May 2).
 - Lecture 11 (Apr. 26): [Neural Networks I](./slides/5-neural-networks/5-neural-networks.pdf)

 - Lecture 12 (Apr. 28): [Neural Networks II (Backpropagation)](./slides/5-neural-networks/5-backpropagation.pdf)
 
 - Assignment III: [[Notebook]](./assignments/assignment3.ipynb)

   <!---  
  Assignment III: linear classification
  * Implement linear regression with gradient descent ? 
  * Implement Newton's method and gradient descent for logistic regression: gradient and hessian
  * ASSIGMENT III BDA: https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb
 https://www.cs.princeton.edu/courses/archive/spring19/cos324/files/logistic-regression.pdf

  ************
  https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb
  https://github.com/syaning/stanford-machine-learning/blob/master/machine-learning-ex2/ex2-py/costFunction.py
  file:///Users/bsouhaib/Dropbox/_TEACHING/UMONS/2019-2020/BDA20/assignments/assignment3_solution.html

  * TWO-DIMENSIONAL PROBLEMS
    * Linear regression using batch gradient descent + Normal equations (+ Generalized inverse)
    * Batch Gradient Descent for Softmax Regression + GRADIENT DESCENT FOR MULTI-CLASS CLASSIFICATION

  ************
  --->

 <!--- BACKPROPAGATION --->
 <!--- DEEP NETS - Universal Approximation Theorem --->
 <!--- Advanced NN (architectectures, regularization, variational bayes, etc) --->
 <!---  https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-1/ --->
 <!---   https://abidlabs.github.io/Atomic-Experiments/--->
 <!---   SEE OTHERS --->

<!---
- Week 11 (May 3-9).
 - Lab 13 (May. 4): Neural Networks (Regularization?)
 
 - Lecture 13 (May. 5): Neural Networks III

 - Lab 14 (May. 6): ??
 - Assignment IV:


 - Week 12 (May 10-16).
 	- Lecture 14 (May. 10): Neural Networks IV
 	- Lab 15 (May. 11): ??
--->
