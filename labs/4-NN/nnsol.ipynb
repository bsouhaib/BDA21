{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analytics 2020-2021 - Lab 13 : Backpropagation\n",
    "\n",
    "1. Derive backprop equations for a simple neural network\n",
    "2. Implement backprop equations in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the toy dataset\n",
    "\n",
    "In this lab, we examine a classification problem for which the data is not linearly separable. We will implement a neural network and train it using gradient descent, computing gradients using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def make_dataset(num_points):\n",
    "    radius = 5\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Generate positive examples (labeled 1).\n",
    "    for i in range(num_points // 2):\n",
    "        r = np.random.uniform(0, radius*0.5)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels.append(1)\n",
    "        \n",
    "    # Generate negative examples (labeled 0).\n",
    "    for i in range(num_points // 2):\n",
    "        r = np.random.uniform(radius*0.7, radius)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels.append(0)\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    return data, labels\n",
    "    \n",
    "num_data = 500\n",
    "data, labels = make_dataset(num_data)\n",
    "\n",
    "# Note: red indicates a label of 1, blue indicates a label of 0\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network definition\n",
    "\n",
    "We will try to classify this data by training a neural network. As a reminder, our goal is to take as input a two dimensional vector $\\mathbf{x} = [x_1, x_2]^T$ and output $\\text{Pr}(t = 1 | \\mathbf{x})$, where $t$ is the label of the datapoint $\\mathbf{x}$. \n",
    "\n",
    "We will use a neural network with one hidden layer which has three hidden units. The equations describing our neural network are below:\n",
    "\n",
    "$$\\mathbf{g} = \\mathbf{U} \\mathbf{x} + \\mathbf{b}$$\n",
    "$$\\mathbf{h} = \\tanh(\\mathbf{g})$$\n",
    "$$z = \\mathbf{W} \\mathbf{h} + c$$\n",
    "$$y = \\sigma(z)$$\n",
    "\n",
    "In the equations above, $\\mathbf{U} = \\begin{pmatrix} u_{11} & u_{12} \\\\ u_{21} & u_{22} \\\\  u_{31} & u_{32} \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}, \\mathbf{b} = \\begin{pmatrix} b_1  \\\\ b_2 \\\\ b_3 \\end{pmatrix} \\in \\mathbb{R}^3, \\mathbf{W} = \\begin{pmatrix} w_{1} & w_{2} & w_{3} \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 3}, c \\in \\mathbb{R}$ are the parameters of our neural network which we must learn. Notice we are writing $\\mathbf{W}$ as a matrix with one row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the neural network\n",
    "\n",
    "We want our neural network to produce predictions for multiple points efficiently. We can do so by vectorizing over training examples. Let  $\\mathbf{X} = \\begin{pmatrix} x_{11} & x_{12} \\\\ \\vdots   & \\vdots  \\\\  x_{N1} & x_{N2}\n",
    "\\end{pmatrix}$ be a matrix containing $N$ datapoints in separate rows. Then we can vectorize by using:\n",
    "\n",
    "$$\\mathbf{G} = \\mathbf{X}\\mathbf{U}^T + \\mathbf{1}\\mathbf{b}^T$$\n",
    "$$\\mathbf{H} = \\tanh(\\mathbf{G})$$\n",
    "$$\\mathbf{z} =  \\mathbf{H}\\mathbf{W}^T + \\mathbf{1}c$$\n",
    "$$\\mathbf{y} = \\sigma(\\mathbf{z})$$\n",
    "\n",
    "$\\mathbf{G}$, for example, will store each of the three hidden unit values for each datapoint in each corresponding row.\n",
    "\n",
    "We can rewrite in scalar form as:\n",
    "$$g_{ij} = u_{j1} x_{i1} + u_{j2} x_{i2} + b_j$$\n",
    "$$h_{ij} = \\tanh(g_{ij})$$\n",
    "$$z_{i} = w_1 h_{i1} + w_2 h_{i2} + w_{3} h_{i3} + c$$\n",
    "$$y_i = \\sigma(z_i)$$\n",
    "Here, $i$ indexes data points and $j$ indexes hidden units, so $i \\in \\{1, \\dots, N\\}$ and $j \\in \\{1, 2, 3\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, initialize our neural network parameters.\n",
    "params = {}\n",
    "params['U'] = np.random.randn(3, 2)\n",
    "params['b'] = np.zeros(3)\n",
    "params['W'] = np.random.randn(3)\n",
    "params['c'] = 0\n",
    "\n",
    "print(params)\n",
    "\n",
    "# Notice we make use of numpy's broadcasting when adding the bias b.\n",
    "def forward(X, params):    \n",
    "    G = np.dot(X, params['U'].T)  + params['b']\n",
    "    H = np.tanh(G)\n",
    "    z = np.dot(H, params['W'].T) + params['c']\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "col = [\"r\" if l == 0 else \"b\" for l in labels]\n",
    "G = np.dot(data, params['U'].T)  + params['b'] # N x 3\n",
    "H = np.tanh(G)\n",
    "ax.scatter3D(H[:, 0], H[:, 1], H[:, 2], c = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the network's predictions\n",
    "\n",
    "Let's visualize the predictions of our untrained network. As we can see, the network does not succeed at classifying the points without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "\n",
    "points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
    "Y = forward(points, params).reshape(num_points, num_points)\n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We will use the logistic loss function:\n",
    "\n",
    "$$\\mathcal{L}(z, t) = -t \\log(\\sigma(z)) - (1 - t)\\log(1 - \\sigma(z)),$$\n",
    "where $\\sigma(z) = 1/(1+exp(-z))$ and $t$ is the true label.\n",
    "\n",
    "Our cost function is the sum over multiple examples of the loss function, normalized by the number of examples:\n",
    "\n",
    "$$\\mathcal{E}(\\mathbf{z}, \\mathbf{t}) = \\frac{1}{N} \\left[\\sum_{i=1}^N \\mathcal{L}(z_i, t_i)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive backpropagation equations\n",
    "\n",
    "We now derive the backpropagation equations in scalar form and then vectorize on the board.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement backpropagation equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, t, params):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Perform forwards computation.\n",
    "    G = np.dot(X, params['U'].T)  + params['b']\n",
    "    H = np.tanh(G)\n",
    "    z = np.dot(H, params['W'].T) + params['c']\n",
    "    y = sigmoid(z)\n",
    "    loss = (1./N) * np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n",
    "    \n",
    "    # Perform backwards computation.\n",
    "    E_bar = 1\n",
    "    z_bar = (1./N) * (y - t)\n",
    "    W_bar = np.dot(H.T, z_bar)\n",
    "    c_bar = np.dot(z_bar, np.ones(N))\n",
    "    H_bar = np.outer(z_bar, params['W'].T)\n",
    "    G_bar = H_bar * (1 - np.tanh(G)**2)\n",
    "    U_bar = np.dot(G_bar.T, X)\n",
    "    b_bar = np.dot(G_bar.T, np.ones(N))\n",
    "    \n",
    "    # Wrap our gradients in a dictionary.\n",
    "    grads = {}\n",
    "    grads['U'] = U_bar\n",
    "    grads['b'] = b_bar\n",
    "    grads['W'] = W_bar\n",
    "    grads['c'] = c_bar\n",
    "    \n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We can train our network parameters using gradient descent once we have computed derivatives using the backpropagation algorithm. Recall that the gradient descent update rule for a given parameter $p$ and a learning rate $\\alpha$ is:\n",
    "\n",
    "$$p \\gets p - \\eta * \\frac{\\partial \\mathcal{E}}{\\partial p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 5000\n",
    "eta = 1\n",
    "for step in range(num_steps):        \n",
    "    grads, loss = backprop(data, labels, params)\n",
    "    for k in params:\n",
    "        params[k] -= eta * grads[k]\n",
    "\n",
    "    # Print loss every so often.\n",
    "    if step % 50 == 0:\n",
    "        print(\"Step {:3d} | Loss {:3.2f}\".format(step, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "\n",
    "points = np.transpose([np.tile(x1s, len(x2s)), np.repeat(x2s, len(x1s))])\n",
    "Y = forward(points, params).reshape(num_points, num_points)\n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "col = [\"r\" if l == 0 else \"b\" for l in labels]\n",
    "G = np.dot(data, params['U'].T)  + params['b'] # N x 3\n",
    "H = np.tanh(G)\n",
    "ax.scatter3D(H[:, 0], H[:, 1], H[:, 2], c = col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python374jvsc74a57bd0398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}